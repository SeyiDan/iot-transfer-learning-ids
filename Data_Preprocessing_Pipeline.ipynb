{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup & Imports"
      ],
      "metadata": {
        "id": "bvc38o35OI71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9TNFR3P-8Ek"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"TRANSFER LEARNING DATA PREPROCESSING PIPELINE\n",
        "\n",
        "This notebook preprocesses and aligns datasets for transfer learning\n",
        "experiments. Run each checkpoint section, verify files saved to Drive,\n",
        "then proceed to next checkpoint.\n",
        "\n",
        "SOURCE: CICIOMT\n",
        "TARGETS: CIC-IoT, IoT-23\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import resample\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = '/content/drive/My Drive/Project_Final_Submission'\n",
        "RAW_DIR = f'{BASE_DIR}/raw_datasets'\n",
        "PREPROCESSED_DIR = f'{BASE_DIR}/preprocessed_datasets'\n",
        "ALIGNED_DIR = f'{BASE_DIR}/enhanced_aligned_datasets'\n",
        "\n",
        "# Create directories\n",
        "for directory in [BASE_DIR, RAW_DIR, PREPROCESSED_DIR, ALIGNED_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"Created: {directory}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DIRECTORY STRUCTURE READY\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Kaggle & Setup Credentials"
      ],
      "metadata": {
        "id": "tK0fi3EJO34Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=============================================================================\n",
        "KAGGLE SETUP - Manual Credentials\n",
        "=============================================================================\n",
        "\"\"\"\n",
        "# Install kagglehub\n",
        "!pip install -q kagglehub\n",
        "\n",
        "import kagglehub\n",
        "import json\n",
        "\n",
        "# Configure Kaggle credentials\n",
        "print(\"=\"*70)\n",
        "print(\"KAGGLE AUTHENTICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Manual input for credentials\n",
        "print(\"\\nEnter your Kaggle credentials:\")\n",
        "print(\"(Find these at: https://www.kaggle.com/settings â†’ API section)\\n\")\n",
        "\n",
        "kaggle_username = input(\"Enter your Kaggle username: \").strip()\n",
        "kaggle_key = input(\"Enter your Kaggle API key: \").strip()\n",
        "\n",
        "# Create kaggle.json content\n",
        "kaggle_config = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_key\n",
        "}\n",
        "\n",
        "# Create .kaggle directory\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Write kaggle.json file\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_config, f)\n",
        "\n",
        "# Set proper permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"\\nKaggle credentials configured successfully\")\n",
        "print(f\"   Username: {kaggle_username}\")\n",
        "print(f\"   Key: {'*' * (len(kaggle_key) - 4) + kaggle_key[-4:]}\")  # Show last 4 chars only\n",
        "\n",
        "# Test the credentials\n",
        "try:\n",
        "    test_path = kagglehub.dataset_download(\"akashdogra/cic-iot-2023\")\n",
        "    print(\"\\nCredentials verified - test download successful!\")\n",
        "    print(f\"   Test file location: {test_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: {e}\")\n",
        "    print(\" Please check your credentials and try again\")"
      ],
      "metadata": {
        "id": "Ebe37wVu_N5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Raw Datasets"
      ],
      "metadata": {
        "id": "es8MA_L7P7pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "DOWNLOAD RAW DATASETS\n",
        "This will take 10-15 minutes depending on connection speed\n",
        "\n",
        "\"\"\"\n",
        "print(\"=\"*70)\n",
        "print(\"DOWNLOADING DATASETS FROM KAGGLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Download datasets\n",
        "datasets_to_download = {\n",
        "    'CICIOMT': 'limamateus/cic-iomt-2024-wifi-mqtt',\n",
        "    'CIC-IoT': 'akashdogra/cic-iot-2023',\n",
        "    'IoT-23': 'engraqeel/iot23preprocesseddata',\n",
        "    'IDS-2018': 'solarmainframe/ids-intrusion-csv'\n",
        "}\n",
        "\n",
        "downloaded_paths = {}\n",
        "\n",
        "for name, kaggle_path in datasets_to_download.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Downloading {name}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    try:\n",
        "        path = kagglehub.dataset_download(kaggle_path)\n",
        "        downloaded_paths[name] = path\n",
        "        print(f\"{name} downloaded to: {path}\")\n",
        "\n",
        "        # List files in directory\n",
        "        import glob\n",
        "        csv_files = glob.glob(f\"{path}/**/*.csv\", recursive=True)\n",
        "        print(f\"   Found {len(csv_files)} CSV file(s)\")\n",
        "        for csv in csv_files[:3]:  # Show first 3\n",
        "            print(f\"   - {os.path.basename(csv)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {name}: {e}\")\n",
        "        downloaded_paths[name] = None\n",
        "\n",
        "# Save paths for later use\n",
        "with open(f'{RAW_DIR}/download_paths.pkl', 'wb') as f:\n",
        "    pickle.dump(downloaded_paths, f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL DATASETS DOWNLOADED\")\n",
        "print(\"=\"*70)\n",
        "print(\"   Checkpoint 1B complete - paths saved to Drive\")\n",
        "print(\"   You can now stop and restart runtime if needed\")"
      ],
      "metadata": {
        "id": "2DxKlYnC_VvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "Z8npKlfBQHJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PREPROCESSING FUNCTIONS\n",
        "Define all preprocessing utilities\n",
        "\"\"\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear memory and run garbage collection\"\"\"\n",
        "    gc.collect()\n",
        "\n",
        "def load_large_csv(filepath, sample_frac=1.0, chunksize=30000):\n",
        "    \"\"\"Load large CSV in chunks\"\"\"\n",
        "    print(f\"Loading: {os.path.basename(filepath)}\")\n",
        "    chunks = []\n",
        "    total_rows = 0\n",
        "\n",
        "    for i, chunk in enumerate(pd.read_csv(filepath, chunksize=chunksize, low_memory=False)):\n",
        "        if sample_frac < 1.0 and np.random.random() > sample_frac:\n",
        "            continue\n",
        "        chunks.append(chunk)\n",
        "        total_rows += len(chunk)\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  Processed {i+1} chunks, {total_rows:,} rows\")\n",
        "\n",
        "    df = pd.concat(chunks, ignore_index=True)\n",
        "    print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
        "    clear_memory()\n",
        "    return df\n",
        "\n",
        "def create_security_features(df):\n",
        "    \"\"\"Create cybersecurity-specific features\"\"\"\n",
        "    print(\"Creating security features...\")\n",
        "\n",
        "    # Find relevant columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Packet-related features\n",
        "    packet_cols = [c for c in numeric_cols if 'packet' in c.lower() or 'pkt' in c.lower()]\n",
        "    byte_cols = [c for c in numeric_cols if 'byte' in c.lower() or 'length' in c.lower()]\n",
        "    duration_cols = [c for c in numeric_cols if 'duration' in c.lower() or 'time' in c.lower()]\n",
        "\n",
        "    new_features = []\n",
        "\n",
        "    # Packet rate features\n",
        "    if len(packet_cols) > 0 and len(duration_cols) > 0:\n",
        "        total_packets = df[packet_cols].sum(axis=1)\n",
        "        duration = df[duration_cols].iloc[:, 0].replace(0, 1)\n",
        "        df['packet_rate'] = total_packets / duration\n",
        "        new_features.append('packet_rate')\n",
        "\n",
        "    # Byte rate features\n",
        "    if len(byte_cols) > 0 and len(duration_cols) > 0:\n",
        "        total_bytes = df[byte_cols].sum(axis=1)\n",
        "        duration = df[duration_cols].iloc[:, 0].replace(0, 1)\n",
        "        df['byte_rate'] = total_bytes / duration\n",
        "        new_features.append('byte_rate')\n",
        "\n",
        "    # Average packet size\n",
        "    if len(byte_cols) > 0 and len(packet_cols) > 0:\n",
        "        total_bytes = df[byte_cols].sum(axis=1)\n",
        "        total_packets = df[packet_cols].sum(axis=1).replace(0, 1)\n",
        "        df['avg_packet_size'] = total_bytes / total_packets\n",
        "        new_features.append('avg_packet_size')\n",
        "\n",
        "    # Flow asymmetry (forward vs backward)\n",
        "    fwd_cols = [c for c in numeric_cols if 'fwd' in c.lower() or 'forward' in c.lower()]\n",
        "    bwd_cols = [c for c in numeric_cols if 'bwd' in c.lower() or 'backward' in c.lower()]\n",
        "\n",
        "    if len(fwd_cols) > 0 and len(bwd_cols) > 0:\n",
        "        fwd_total = df[fwd_cols].sum(axis=1)\n",
        "        bwd_total = df[bwd_cols].sum(axis=1)\n",
        "        df['flow_asymmetry'] = (fwd_total - bwd_total) / (fwd_total + bwd_total + 1)\n",
        "        new_features.append('flow_asymmetry')\n",
        "\n",
        "    # Fill NaN and infinity\n",
        "    for feat in new_features:\n",
        "        df[feat] = df[feat].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    print(f\"Created {len(new_features)} security features\")\n",
        "    return df, new_features\n",
        "\n",
        "def create_statistical_features(df):\n",
        "    \"\"\"Create statistical aggregation features\"\"\"\n",
        "    print(\"Creating statistical features...\")\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Select subset of columns to avoid explosion\n",
        "    selected_cols = numeric_cols[:15]  # Use first 15 numeric columns\n",
        "\n",
        "    new_features = []\n",
        "\n",
        "    # Mean and std across selected features\n",
        "    df['flow_mean'] = df[selected_cols].mean(axis=1)\n",
        "    df['flow_std'] = df[selected_cols].std(axis=1)\n",
        "    df['flow_min'] = df[selected_cols].min(axis=1)\n",
        "    df['flow_max'] = df[selected_cols].max(axis=1)\n",
        "\n",
        "    new_features.extend(['flow_mean', 'flow_std', 'flow_min', 'flow_max'])\n",
        "\n",
        "    # Fill NaN\n",
        "    for feat in new_features:\n",
        "        df[feat] = df[feat].fillna(0)\n",
        "\n",
        "    print(f\"Created {len(new_features)} statistical features\")\n",
        "    return df, new_features\n",
        "\n",
        "def map_to_standard_categories(label):\n",
        "    \"\"\"Map dataset-specific labels to standard categories\"\"\"\n",
        "    label = str(label).lower()\n",
        "\n",
        "    # DoS/DDoS patterns\n",
        "    if any(x in label for x in ['dos', 'ddos', 'flood', 'slowloris']):\n",
        "        return 'DoS'\n",
        "\n",
        "    # Botnet/Malware\n",
        "    if any(x in label for x in ['botnet', 'mirai', 'c&c', 'okiru', 'torii', 'malware']):\n",
        "        return 'Botnet'\n",
        "\n",
        "    # Reconnaissance\n",
        "    if any(x in label for x in ['scan', 'recon', 'reconnaissance', 'probe']):\n",
        "        return 'Reconnaissance'\n",
        "\n",
        "    # Benign\n",
        "    if 'benign' in label or 'normal' in label:\n",
        "        return 'Benign'\n",
        "\n",
        "    # Spoofing\n",
        "    if 'spoof' in label or 'arp' in label:\n",
        "        return 'Spoofing'\n",
        "\n",
        "    # Web attacks\n",
        "    if any(x in label for x in ['xss', 'sql', 'injection', 'web']):\n",
        "        return 'WebAttack'\n",
        "\n",
        "    # Brute force\n",
        "    if 'brute' in label or 'dictionary' in label:\n",
        "        return 'BruteForce'\n",
        "\n",
        "    # Default\n",
        "    return 'Other'\n",
        "\n",
        "def balance_classes(X, y, max_samples=10000, min_samples=100):\n",
        "    \"\"\"Balance classes using hybrid approach\"\"\"\n",
        "    print(f\"\\nBalancing classes (max={max_samples}, min={min_samples})...\")\n",
        "\n",
        "    unique_classes = y.unique()\n",
        "    balanced_dfs = []\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        mask = (y == cls)\n",
        "        X_cls = X[mask]\n",
        "        y_cls = y[mask]\n",
        "        n_samples = len(y_cls)\n",
        "\n",
        "        print(f\"  {cls}: {n_samples} samples\", end='')\n",
        "\n",
        "        if n_samples > max_samples:\n",
        "            # Downsample\n",
        "            indices = resample(range(n_samples), n_samples=max_samples,\n",
        "                             random_state=42, replace=False)\n",
        "            X_cls = X_cls.iloc[indices] if isinstance(X_cls, pd.DataFrame) else X_cls[indices]\n",
        "            y_cls = y_cls.iloc[indices] if isinstance(y_cls, pd.Series) else y_cls[indices]\n",
        "            print(f\" -> downsampled to {max_samples}\")\n",
        "        elif n_samples < min_samples:\n",
        "            # Remove rare class\n",
        "            print(f\" -> removed (too few samples)\")\n",
        "            continue\n",
        "        else:\n",
        "            print(f\" -> kept as is\")\n",
        "\n",
        "        balanced_dfs.append((X_cls, y_cls))\n",
        "\n",
        "    # Combine\n",
        "    if len(balanced_dfs) == 0:\n",
        "        raise ValueError(\"No classes remaining after balancing\")\n",
        "\n",
        "    X_balanced = pd.concat([x for x, _ in balanced_dfs], ignore_index=True)\n",
        "    y_balanced = pd.concat([y for _, y in balanced_dfs], ignore_index=True)\n",
        "\n",
        "    print(f\"\\nBalanced: {len(X_balanced)} total samples, {len(unique_classes)} classes\")\n",
        "    return X_balanced, y_balanced\n",
        "\n",
        "def preprocess_dataset(csv_path, dataset_name, sample_frac=0.3):\n",
        "    \"\"\"Complete preprocessing pipeline for a dataset\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"PREPROCESSING: {dataset_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load data\n",
        "    df = load_large_csv(csv_path, sample_frac=sample_frac)\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    # Identify label column\n",
        "    label_col = None\n",
        "    for col in ['label', 'Label', 'attack_type', 'Attack', 'class']:\n",
        "        if col in df.columns:\n",
        "            label_col = col\n",
        "            break\n",
        "\n",
        "    if label_col is None:\n",
        "        # Try last column\n",
        "        label_col = df.columns[-1]\n",
        "        print(f\"No standard label column found, using: {label_col}\")\n",
        "\n",
        "    # Separate features and labels\n",
        "    y = df[label_col]\n",
        "    X = df.drop(columns=[label_col])\n",
        "\n",
        "    print(f\"Features: {X.shape[1]}, Samples: {len(y)}\")\n",
        "    print(f\"Original classes: {y.nunique()}\")\n",
        "\n",
        "    # Map to standard categories\n",
        "    print(\"\\nMapping labels to standard categories...\")\n",
        "    y = y.apply(map_to_standard_categories)\n",
        "    print(f\"Mapped to {y.nunique()} standard categories:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    # Drop non-numeric columns\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    X = X[numeric_cols]\n",
        "    print(f\"\\nNumeric features: {len(numeric_cols)}\")\n",
        "\n",
        "    # Handle missing values\n",
        "    X = X.fillna(0)\n",
        "    X = X.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    # Create engineered features\n",
        "    X, security_features = create_security_features(X)\n",
        "    X, statistical_features = create_statistical_features(X)\n",
        "\n",
        "    print(f\"\\nTotal features after engineering: {X.shape[1]}\")\n",
        "\n",
        "    # Balance classes\n",
        "    X, y = balance_classes(X, y, max_samples=10000, min_samples=100)\n",
        "\n",
        "    # Train/validation/test split\n",
        "    print(\"\\nSplitting into train/val/test...\")\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_temp = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    # Package results\n",
        "    preprocessed_data = {\n",
        "        'train_x': X_train,\n",
        "        'train_y': y_train,\n",
        "        'val_x': X_val,\n",
        "        'val_y': y_val,\n",
        "        'test_x': X_test,\n",
        "        'test_y': y_test,\n",
        "        'feature_names': X.columns.tolist(),\n",
        "        'classes': sorted(y.unique()),\n",
        "        'dataset_name': dataset_name\n",
        "    }\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nPreprocessing complete in {elapsed:.1f}s\")\n",
        "\n",
        "    clear_memory()\n",
        "    return preprocessed_data\n",
        "\n",
        "print(\"Preprocessing functions defined\")"
      ],
      "metadata": {
        "id": "QhemAuzB_ZRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Each Dataset"
      ],
      "metadata": {
        "id": "_RZcgRYTQNED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PREPROCESS ALL DATASETS\n",
        "This takes 60-90 minutes total\n",
        "\"\"\"\n",
        "\n",
        "# Load download paths\n",
        "with open(f'{RAW_DIR}/download_paths.pkl', 'rb') as f:\n",
        "    downloaded_paths = pickle.load(f)\n",
        "\n",
        "import glob\n",
        "\n",
        "# Process each dataset\n",
        "preprocessed_datasets = {}\n",
        "\n",
        "# 1. CICIOMT (Source)\n",
        "print(\"\\n\" + \"\"*35)\n",
        "print(\"PROCESSING 1/4: CICIOMT (SOURCE)\")\n",
        "print(\"\"*35)\n",
        "csv_path = glob.glob(f\"{downloaded_paths['CICIOMT']}/**/*.csv\", recursive=True)[0]\n",
        "ciciomt = preprocess_dataset(csv_path, 'CICIOMT', sample_frac=0.3)\n",
        "preprocessed_datasets['CICIOMT'] = ciciomt\n",
        "\n",
        "# Save checkpoint\n",
        "with open(f'{PREPROCESSED_DIR}/ciciomt_preprocessed.pkl', 'wb') as f:\n",
        "    pickle.dump(ciciomt, f)\n",
        "print(f\"Saved: ciciomt_preprocessed.pkl\")\n",
        "clear_memory()\n",
        "\n",
        "# 2. CIC-IoT (Target 1)\n",
        "print(\"\\n\" + \"\"*35)\n",
        "print(\"PROCESSING 2/4: CIC-IoT (TARGET 1)\")\n",
        "print(\"\"*35)\n",
        "csv_path = glob.glob(f\"{downloaded_paths['CIC-IoT']}/**/*.csv\", recursive=True)[0]\n",
        "ciciot = preprocess_dataset(csv_path, 'CIC-IoT', sample_frac=0.3)\n",
        "preprocessed_datasets['CIC-IoT'] = ciciot\n",
        "\n",
        "# Save checkpoint\n",
        "with open(f'{PREPROCESSED_DIR}/ciciot_preprocessed.pkl', 'wb') as f:\n",
        "    pickle.dump(ciciot, f)\n",
        "print(f\"Saved: ciciot_preprocessed.pkl\")\n",
        "clear_memory()\n",
        "\n",
        "# 3. IoT-23 (Target 2)\n",
        "print(\"\\n\" + \"\"*35)\n",
        "print(\"PROCESSING 3/4: IoT-23 (TARGET 2)\")\n",
        "print(\"\"*35)\n",
        "csv_path = glob.glob(f\"{downloaded_paths['IoT-23']}/**/*.csv\", recursive=True)[0]\n",
        "iot23 = preprocess_dataset(csv_path, 'IoT-23', sample_frac=0.3)\n",
        "preprocessed_datasets['IoT-23'] = iot23\n",
        "\n",
        "# Save checkpoint\n",
        "with open(f'{PREPROCESSED_DIR}/iot23_preprocessed.pkl', 'wb') as f:\n",
        "    pickle.dump(iot23, f)\n",
        "print(f\"Saved: iot23_preprocessed.pkl\")\n",
        "clear_memory()\n",
        "\n",
        "# 4. IDS-2018 (Target 3)\n",
        "print(\"\\n\" + \"\"*35)\n",
        "print(\"PROCESSING 4/4: IDS-2018 (TARGET 3)\")\n",
        "print(\"\"*35)\n",
        "csv_path = glob.glob(f\"{downloaded_paths['IDS-2018']}/**/*.csv\", recursive=True)[0]\n",
        "ids2018 = preprocess_dataset(csv_path, 'IDS-2018', sample_frac=0.3)\n",
        "preprocessed_datasets['IDS-2018'] = ids2018\n",
        "\n",
        "# Save checkpoint\n",
        "with open(f'{PREPROCESSED_DIR}/ids2018_preprocessed.pkl', 'wb') as f:\n",
        "    pickle.dump(ids2018, f)\n",
        "print(f\"Saved: ids2018_preprocessed.pkl\")\n",
        "clear_memory()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHECKPOINT 3 COMPLETE - ALL DATASETS PREPROCESSED\")\n",
        "print(\"=\"*70)\n",
        "print(\"   All preprocessed files saved to Drive\")\n",
        "print(\"   You can now stop and restart runtime if needed\")"
      ],
      "metadata": {
        "id": "sSt_PMk1_cDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Alignment Functions"
      ],
      "metadata": {
        "id": "JtYmXHuMQ4EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"FEATURE ALIGNMENT FUNCTIONS\"\"\"\n",
        "\n",
        "def align_features_pairwise(source_data, target_data):\n",
        "    \"\"\"\n",
        "    Align source and target datasets for transfer learning\n",
        "    Uses common features + engineered features + PCA for missing dimensions\n",
        "    \"\"\"\n",
        "    print(f\"\\nAligning {source_data['dataset_name']} -> {target_data['dataset_name']}\")\n",
        "\n",
        "    # Get feature sets\n",
        "    source_features = set(source_data['train_x'].columns)\n",
        "    target_features = set(target_data['train_x'].columns)\n",
        "\n",
        "    # Find common features\n",
        "    common_features = sorted(list(source_features & target_features))\n",
        "    print(f\"  Common features: {len(common_features)}\")\n",
        "\n",
        "    # Extract common features\n",
        "    source_train = source_data['train_x'][common_features]\n",
        "    target_train = target_data['train_x'][common_features]\n",
        "    target_val = target_data['val_x'][common_features]\n",
        "    target_test = target_data['test_x'][common_features]\n",
        "\n",
        "    # Add PCA features if needed (for dimension matching)\n",
        "    n_pca = min(15, len(common_features))\n",
        "    if n_pca > 0:\n",
        "        print(f\"  Adding {n_pca} PCA features...\")\n",
        "        pca = PCA(n_components=n_pca, random_state=42)\n",
        "\n",
        "        # Fit on source\n",
        "        source_pca = pca.fit_transform(source_train)\n",
        "        pca_cols = [f'pca_{i}' for i in range(n_pca)]\n",
        "\n",
        "        # Transform target\n",
        "        target_train_pca = pca.transform(target_train)\n",
        "        target_val_pca = pca.transform(target_val)\n",
        "        target_test_pca = pca.transform(target_test)\n",
        "\n",
        "        # Add to dataframes\n",
        "        for i, col in enumerate(pca_cols):\n",
        "            source_train[col] = source_pca[:, i]\n",
        "            target_train[col] = target_train_pca[:, i]\n",
        "            target_val[col] = target_val_pca[:, i]\n",
        "            target_test[col] = target_test_pca[:, i]\n",
        "\n",
        "    # Scale features\n",
        "    print(\"  Scaling features...\")\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    source_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(source_train),\n",
        "        columns=source_train.columns,\n",
        "        index=source_train.index\n",
        "    )\n",
        "\n",
        "    target_train_scaled = pd.DataFrame(\n",
        "        scaler.transform(target_train),\n",
        "        columns=target_train.columns,\n",
        "        index=target_train.index\n",
        "    )\n",
        "\n",
        "    target_val_scaled = pd.DataFrame(\n",
        "        scaler.transform(target_val),\n",
        "        columns=target_val.columns,\n",
        "        index=target_val.index\n",
        "    )\n",
        "\n",
        "    target_test_scaled = pd.DataFrame(\n",
        "        scaler.transform(target_test),\n",
        "        columns=target_test.columns,\n",
        "        index=target_test.index\n",
        "    )\n",
        "\n",
        "    # Package aligned source\n",
        "    aligned_source = {\n",
        "        'train_x': source_train_scaled,\n",
        "        'train_y': source_data['train_y'],\n",
        "        'val_x': source_data['val_x'][common_features],  # Keep for validation\n",
        "        'val_y': source_data['val_y'],\n",
        "        'feature_names': source_train_scaled.columns.tolist(),\n",
        "        'classes': source_data['classes'],\n",
        "        'dataset_name': source_data['dataset_name']\n",
        "    }\n",
        "\n",
        "    # Package aligned target\n",
        "    aligned_target = {\n",
        "        'train_x': target_train_scaled,\n",
        "        'train_y': target_data['train_y'],\n",
        "        'val_x': target_val_scaled,\n",
        "        'val_y': target_data['val_y'],\n",
        "        'test_x': target_test_scaled,\n",
        "        'test_y': target_data['test_y'],\n",
        "        'feature_names': target_test_scaled.columns.tolist(),\n",
        "        'classes': target_data['classes'],\n",
        "        'dataset_name': target_data['dataset_name']\n",
        "    }\n",
        "\n",
        "    print(f\"Alignment complete: {len(aligned_source['train_x'].columns)} features\")\n",
        "\n",
        "    return aligned_source, aligned_target\n",
        "\n",
        "print(\"Alignment functions defined\")"
      ],
      "metadata": {
        "id": "wm6J84B7Q7zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Aligned Datasets"
      ],
      "metadata": {
        "id": "tyfc1SOQRLxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CREATE PAIRWISE ALIGNED DATASETS\n",
        "This creates transfer learning pairs: CICIOMT -> each target\"\"\"\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING ALIGNED DATASETS FOR TRANSFER LEARNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load preprocessed data (if not in memory from checkpoint 3)\n",
        "try:\n",
        "    if 'preprocessed_datasets' not in locals():\n",
        "        print(\"Loading preprocessed datasets from Drive...\")\n",
        "        preprocessed_datasets = {}\n",
        "\n",
        "        with open(f'{PREPROCESSED_DIR}/ciciomt_preprocessed.pkl', 'rb') as f:\n",
        "            preprocessed_datasets['CICIOMT'] = pickle.load(f)\n",
        "        with open(f'{PREPROCESSED_DIR}/ciciot_preprocessed.pkl', 'rb') as f:\n",
        "            preprocessed_datasets['CIC-IoT'] = pickle.load(f)\n",
        "        with open(f'{PREPROCESSED_DIR}/iot23_preprocessed.pkl', 'rb') as f:\n",
        "            preprocessed_datasets['IoT-23'] = pickle.load(f)\n",
        "        with open(f'{PREPROCESSED_DIR}/ids2018_preprocessed.pkl', 'rb') as f:\n",
        "            preprocessed_datasets['IDS-2018'] = pickle.load(f)\n",
        "        print(\"All datasets loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading preprocessed data: {e}\")\n",
        "    print(\"Make sure Checkpoint 3 completed successfully\")\n",
        "\n",
        "# Create aligned pairs\n",
        "ciciomt_source = preprocessed_datasets['CICIOMT']\n",
        "targets = ['CIC-IoT', 'IoT-23', 'IDS-2018']\n",
        "\n",
        "for target_name in targets:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ALIGNING: CICIOMT -> {target_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    target_data = preprocessed_datasets[target_name]\n",
        "\n",
        "    # Create alignment\n",
        "    aligned_source, aligned_target = align_features_pairwise(ciciomt_source, target_data)\n",
        "\n",
        "    # Save aligned datasets\n",
        "    source_filename = f'{ALIGNED_DIR}/enhanced_aligned_ciciomt_for_{target_name.lower()}.pkl'\n",
        "    target_filename = f'{ALIGNED_DIR}/enhanced_aligned_{target_name.lower()}.pkl'\n",
        "\n",
        "    with open(source_filename, 'wb') as f:\n",
        "        pickle.dump(aligned_source, f)\n",
        "    print(f\"Saved: {os.path.basename(source_filename)}\")\n",
        "\n",
        "    with open(target_filename, 'wb') as f:\n",
        "        pickle.dump(aligned_target, f)\n",
        "    print(f\"Saved: {os.path.basename(target_filename)}\")\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHECKPOINT 5 COMPLETE - ALL ALIGNMENTS CREATED\")\n",
        "print(\"=\"*70)\n",
        "print(\"Enhanced aligned datasets saved to Drive\")\n",
        "print(\"Ready for transfer learning experiments (Notebook 2)\")"
      ],
      "metadata": {
        "id": "KCLF6SRlRLfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verification & Summary"
      ],
      "metadata": {
        "id": "D23CATXURbZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "VERIFY ALL FILES AND GENERATE SUMMARY\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check all files exist\n",
        "print(\"\\n1  PREPROCESSED DATASETS:\")\n",
        "for name in ['ciciomt', 'ciciot', 'iot23', 'ids2018']:\n",
        "    filepath = f'{PREPROCESSED_DIR}/{name}_preprocessed.pkl'\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024**2)\n",
        "        print(f\"   {name}_preprocessed.pkl ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"   {name}_preprocessed.pkl (MISSING)\")\n",
        "\n",
        "print(\"\\n2  ALIGNED DATASETS:\")\n",
        "for target in ['cic-iot', 'iot-23', 'ids-2018']:\n",
        "    source_file = f'{ALIGNED_DIR}/enhanced_aligned_ciciomt_for_{target}.pkl'\n",
        "    target_file = f'{ALIGNED_DIR}/enhanced_aligned_{target}.pkl'\n",
        "\n",
        "    if os.path.exists(source_file) and os.path.exists(target_file):\n",
        "        source_size = os.path.getsize(source_file) / (1024**2)\n",
        "        target_size = os.path.getsize(target_file) / (1024**2)\n",
        "        print(f\"   Pair: CICIOMT -> {target.upper()} ({source_size:.1f} MB + {target_size:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"   Pair: CICIOMT -> {target.upper()} (MISSING)\")\n",
        "\n",
        "print(\"\\n3  DATASET DETAILS:\")\n",
        "for name in ['CICIOMT', 'CIC-IoT', 'IoT-23', 'IDS-2018']:\n",
        "    try:\n",
        "        if name == 'CICIOMT':\n",
        "            filepath = f'{PREPROCESSED_DIR}/ciciomt_preprocessed.pkl'\n",
        "        elif name == 'CIC-IoT':\n",
        "            filepath = f'{PREPROCESSED_DIR}/ciciot_preprocessed.pkl'\n",
        "        elif name == 'IoT-23':\n",
        "            filepath = f'{PREPROCESSED_DIR}/iot23_preprocessed.pkl'\n",
        "        else:\n",
        "            filepath = f'{PREPROCESSED_DIR}/ids2018_preprocessed.pkl'\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        print(f\"\\n   {name}:\")\n",
        "        print(f\"      Train: {len(data['train_x']):,} samples\")\n",
        "        print(f\"      Test: {len(data['test_x']):,} samples\")\n",
        "        print(f\"      Features: {len(data['feature_names'])}\")\n",
        "        print(f\"      Classes: {', '.join(data['classes'])}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   {name}: Error loading - {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAll datasets preprocessed and aligned\")\n",
        "print(\"Files saved to Google Drive\")\n",
        "print(\"Ready for Notebook 2 (Transfer Learning Training)\")\n",
        "print(\"\\nLocation: /content/drive/My Drive/transfer_learning_project/\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "10SfpHcVRgTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hg81QmhYRbJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}